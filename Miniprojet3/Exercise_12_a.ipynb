{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set() #sets the matplotlib style to seaborn style\n",
    "\n",
    "from scipy.io import loadmat \n",
    "from scipy.ndimage import convolve1d\n",
    "from scipy.signal import butter\n",
    "from scipy.signal import sosfiltfilt\n",
    "from scipy.signal import welch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, we performed classification and we managed achieved decent results. This week, we will use a Ninapro dataset to explore machine learning concepts to better evaluate our model and feature selections methods.\n",
    "\n",
    "Now, lets load the data and start with a simple classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('exercise_1__dataset_1.mat')\n",
    "emg = data['emg']\n",
    "stimulus = data['restimulus']\n",
    "repetition = data['rerepetition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as week 10, for each trial (one repetition of one stimulus) we create a nested list of envelopes of the trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stimuli = len(np.unique(stimulus)) - 1 # -1 because 0 is the resting condition\n",
    "n_repetitions = len(np.unique(repetition)) - 1 # -1 because 0 is not a repetition\n",
    "\n",
    "mov_mean_length = 25\n",
    "mov_mean_weights = np.ones(mov_mean_length) / mov_mean_length\n",
    "\n",
    "#initializing the data structure\n",
    "emg_windows = [[None for j in range(n_repetitions)] for i in range(n_stimuli)]\n",
    "emg_rectified = [[None for j in range(n_repetitions)] for i in range(n_stimuli)]\n",
    "emg_envelopes = [[None for j in range(n_repetitions)] for i in range(n_stimuli)]\n",
    "\n",
    "for i in range(n_stimuli):\n",
    "    for j in range(n_repetitions):\n",
    "        idx = np.logical_and(stimulus == i + 1, repetition == j + 1).flatten()\n",
    "        emg_windows[i][j] = emg[idx, :]\n",
    "        emg_rectified[i][j] = np.abs(emg_windows[i][j])\n",
    "        # emg_rectified = np.abs(emg_windows[i][j])\n",
    "        emg_envelopes[i][j] = convolve1d(emg_rectified[i][j], mov_mean_weights, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, using the provided code, we extract the features. In this simple example we will look at mean and standard deviation within the full window of each trial. Feel free to add in more features learnt last week that you think can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_from_ninapro(emg, stimulus, repetition, features=None):\n",
    "    n_stimuli = np.unique(stimulus).size - 1\n",
    "    n_repetitions = np.unique(repetition).size - 1\n",
    "    n_samples = n_stimuli * n_repetitions\n",
    "    \n",
    "    n_channels = emg.shape[1]\n",
    "    n_features = 0\n",
    "    if \"mean\" in features:\n",
    "        n_features += n_channels\n",
    "    if \"std\" in features:\n",
    "        n_features += n_channels\n",
    "    \n",
    "    dataset = np.zeros((n_samples, n_features))\n",
    "    labels = np.zeros(n_samples)\n",
    "    current_sample_index = 0\n",
    "    for i in range(n_stimuli):\n",
    "        for j in range(n_repetitions):\n",
    "            labels[current_sample_index] = i + 1\n",
    "            current_feature_index = 0\n",
    "            selected_tsteps = np.logical_and(stimulus == i + 1, repetition == j + 1).squeeze()\n",
    "            \n",
    "            if \"mean\" in features:\n",
    "                selected_features = np.arange(current_feature_index, current_feature_index + n_channels)\n",
    "                dataset[current_sample_index, selected_features] = np.mean(emg[selected_tsteps, :], axis=0)\n",
    "                current_feature_index += n_channels\n",
    "            if \"std\" in features:\n",
    "                selected_features = np.arange(current_feature_index, current_feature_index + n_channels)\n",
    "                dataset[current_sample_index, selected_features] = np.std(emg[selected_tsteps, :], axis=0)\n",
    "                current_feature_index += n_channels\n",
    "\n",
    "            current_sample_index += 1\n",
    "            \n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = build_dataset_from_ninapro(\n",
    "    emg=emg,\n",
    "    stimulus=stimulus,\n",
    "    repetition=repetition,\n",
    "    features=[\"mean\", \"std\"]\n",
    ")\n",
    "X = dataset\n",
    "y = labels\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 12 actions and 10 repeats for each actions, we have a total of 120 training examples. This is a small classification problem.\n",
    "\n",
    "Now, lets apply the classification techniques we learnt last week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train)\n",
    "X_test_z = scaler.transform(X_test)\n",
    "\n",
    "# train a classifier on the normalized data\n",
    "clf = SVC()\n",
    "clf.fit(X_train_z, y_train)\n",
    "\n",
    "# evaluate the classifier on the test set\n",
    "y_pred = clf.predict(X_test_z)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "confmat = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(confmat, annot=True, ax=ax)\n",
    "ax.set_ylabel(\"True label\")\n",
    "ax.set_xlabel(\"Predicted label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation for evaluation\n",
    "\n",
    "In the previous example, we used a random method to split the data into training and testing sets. It's important to note that the model's performance can vary significantly depending on this split. \n",
    "\n",
    "Therefore, in our next step, we plan to vary the composition of the training and testing datasets and train multiple models accordingly. This approach is known as cross-validation, and it helps in assessing the model's effectiveness more reliably by using different data subsets for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = ???\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, the 5 different model is trained and evaluated. Each model is trained on a different training and testing set. We see that depending on how the data is split, the performance could vary from 0.64 to 0.76. Hence, we can then use the average performance for our evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "Something that we can do to improve classification is also to optimize the parameters of the models. For instance, in the SVC function on sklearn, we have the option to select various values for 'C', 'gamma', and the kernel. To effectively test the different combinations of these parameters, we can utilize the GridSearchCV function of sklearn, which is designed to methodically identify the optimal combination of these parameters, thereby ensuring the best possible performance and accuracy for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross validation to find the best hyperparameters for SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"gamma\": [1, 0.1, 0.01, 0.001],\n",
    "    \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"]\n",
    "}\n",
    "\n",
    "grid = ???\n",
    "grid.fit(X_train_z, y_train)\n",
    "\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "y_pred = grid.predict(X_test_z)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how the accuracy increased by searching for the best parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance/selection\n",
    "\n",
    "In this example, we are dealing with a relatively small set of 20 features, comprising the mean and standard deviation of each channel. However, in scenarios like exercise 11, where we encounter a larger number of features, it can be important to employ feature selection techniques to improve model performance and efficiency. \n",
    "\n",
    "For this purpose, we propose a method that utilizes functions like mutual_info_classif and SelectKBest from sklearn. mutual_info_classif evaluates the mutual information between each feature and the target variable, providing an insight into the relevance of each feature. Meanwhile, SelectKBest allows us to select a specified number of features that have the highest scores according to a given scoring function, in this case, the mutual information. By combining these two functions, we can effectively reduce the feature space to those most relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "mutual_info = ???\n",
    "print(mutual_info)\n",
    "\n",
    "k_best = ???\n",
    "k_best.fit(X_train_z, y_train)\n",
    "\n",
    "X_train_best = k_best.transform(X_train_z)\n",
    "X_test_best = k_best.transform(X_test_z)\n",
    "\n",
    "clf = SVC(**grid.best_params_) # use the best parameters found before\n",
    "clf.fit(X_train_best, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_best)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "confmat = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(confmat, annot=True, ax=ax)\n",
    "ax.set_ylabel(\"True label\")\n",
    "ax.set_xlabel(\"Predicted label\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goldin_3090",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
